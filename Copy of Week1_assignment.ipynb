{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R0wJgz48mu08Q_teQL5MgFMBjbFmP3wX","timestamp":1765366130364}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Part 1: Text Preprocessing**"],"metadata":{"id":"IYACLCwsyxfa"}},{"cell_type":"markdown","source":["\n","\n","**Data-** given in the below code cell\n","\n","**1.1: Preprocessing From Scratch**\n","\n","**Goal:** Write a function clean_text_scratch(text) that performs the following without using NLTK or Spacy:\n","\n","1. Lowercasing: Convert text to lowercase.\n","\n","2. Punctuation Removal: Use Python's re (regex) library or string methods to remove special characters (!, ., ,, :, ;, ..., ').\n","\n","3. Tokenization: Split the string into a list of words based on whitespace.\n","\n","4. Stopword Removal: Filter out words found in this list: ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or'].\n","\n","5. Simple Stemming: Create a helper function that removes suffixes 'ing', 'ly', 'ed', and 's' from the end of words.\n","\n","\n","Note: This is a \"Naive\" stemmer. It will break words like \"sing\" -> \"s\". This illustrates why we need libraries!\n","\n","**Task:** Run this function on the first sentence of the corpus and print the result."],"metadata":{"id":"MTP8EqylwqDf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIRv3qS2bTFt"},"outputs":[],"source":["corpus = [\n","    \"Artificial Intelligence is transforming the world; however, ethical concerns remain!\",\n","    \"The pizza was absolutely delicious, but the service was terrible ... I won't go back.\",\n","    \"The quick brown fox jumps over the lazy dog.\",\n","    \"To be, or not to be, that is the question: Whether 'tis nobler in the mind.\",\n","    \"Data science involves statistics, linear algebra, and machine learning.\",\n","    \"I love machine learning, but I hate the math behind it.\"\n","]"]},{"cell_type":"code","source":["import re\n","\n","stop_words = ['the', 'is', 'in', 'to', 'of', 'and', 'a', 'it', 'was', 'but', 'or']\n","\n","def simple_stemmer(word):\n","    if word.endswith('ing'):\n","        return word[:-3]\n","    elif word.endswith('ly'):\n","        return word[:-2]\n","    elif word.endswith('ed'):\n","        return word[:-2]\n","    elif word.endswith('s'):\n","        return word[:-1]\n","    return word\n","\n","def clean_text_scratch(text):\n","    # 1. Lowercasing\n","    text = text.lower()\n","\n","    # 2. Punctuation rem\n","    text = re.sub(r'[!.,;:...\\[\\]‘’\\-]', '', text)\n","\n","    # 3. tokenization\n","    tokens = text.split()\n","\n","    # 4. Stopwordremoval\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # 5. stemming\n","    tokens = [simple_stemmer(word) for word in tokens]\n","\n","    return tokens\n","\n","#running on first line of corpus\n","sentence1 = corpus[0]\n","cleaned_sentence1= clean_text_scratch(sentence1)\n","print(cleaned_sentence1)\n"],"metadata":{"id":"oR4BKqITy17z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765360573083,"user_tz":-330,"elapsed":10,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"97212ccb-52ef-4dc7-d8bf-bd25008dee83"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["['artificial', 'intelligence', 'transform', 'world', 'however', 'ethical', 'concern', 'remain']\n"]}]},{"cell_type":"markdown","source":["**1.2: Preprocessing Using Tools**\n","\n","**Goal:** Use the nltk library to perform the same cleaning on the entire corpus.\n","\n","**Steps:**\n","\n","1. Use nltk.tokenize.word_tokenize.\n","2. Use nltk.corpus.stopwords.\n","3. Use nltk.stem.WordNetLemmatizer\n","\n","to convert words to their root (e.g., \"jumps\" $\\to$ \"jump\", \"transforming\" $\\to$ \"transform\").\n","\n","\n","**Task:** Print the cleaned, lemmatized tokens for the second sentence (The pizza review)."],"metadata":{"id":"dN9rNq7WycqZ"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt_tab')\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import re # Import re for punctuation removal\n","\n","def clean_text_nltk(text):\n","    # Lowercasing\n","    text = text.lower()\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # punctuation removal\n","    tokens = [word for word in tokens if word.isalpha()]\n","\n","    # stopward removal\n","    stop_words_nltk = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words_nltk]\n","\n","    # Lemmatiza.\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    return tokens\n","\n","#getting setence from corpus\n","sentence2 = corpus[1]\n","\n","# Clean  second sentence\n","cleaned_sentence2 = clean_text_nltk(sentence2)\n","\n","print(cleaned_sentence2)\n"],"metadata":{"id":"v_4FjuCqy5Kt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765361157635,"user_tz":-330,"elapsed":7861,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"2d437249-d786-4efa-920c-8462947d8935"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['pizza', 'absolutely', 'delicious', 'service', 'terrible', 'wo', 'go', 'back']\n"]}]},{"cell_type":"markdown","source":["# **Part 2: Text Representation**"],"metadata":{"id":"hPMrwva2y1LG"}},{"cell_type":"markdown","source":["**2.1: Bag of Words (BoW)**\n","\n","**Logic:**\n","\n","**Build Vocabulary:** Create a list of all unique words in the entire corpus (after cleaning). Sort them alphabetically.\n","\n","**Vectorize:** Write a function that takes a sentence and returns a list of numbers. Each number represents the count of a vocabulary word in that sentence.\n","\n","**Task:** Print the unique Vocabulary list. Then, print the BoW vector for: \"The quick brown fox jumps over the lazy dog.\""],"metadata":{"id":"cKa8NnZ5zLlm"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0451b455","executionInfo":{"status":"ok","timestamp":1765361907197,"user_tz":-330,"elapsed":59,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"bc4cf56d-639f-4fca-f032-b4548bb70223"},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# NLTK downloads (run once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","def clean_text_nltk(text):\n","\n","    text = text.lower()\n","\n","    tokens = word_tokenize(text)\n","\n","    tokens = [word for word in tokens if word.isalpha()]\n","\n","    stop_words_nltk = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words_nltk]\n","\n","\n","    lemmatizer = WordNetLemmatizer()\n","\n","\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    return tokens\n","\n","# Clean whole corpus\n","cleaned_corpus = []\n","for sentence in corpus:\n","    cleaned_corpus.extend(clean_text_nltk(sentence))\n","\n","# Build vocablist\n","vocab = sorted(list(set(cleaned_corpus)))\n","print(\"Unique VocabList:\", vocab)\n","\n","# Function to vectorize a sentence\n","def vectorize_bow(sentence, vocab):\n","    sentence_tokens = clean_text_nltk(sentence)\n","\n","    vector = [0] * len(vocab)\n","\n","    for token in sentence_tokens:\n","        if token in vocab:\n","         vector[vocab.index(token)] += 1\n","    return vector\n","\n","# The sentence to vectorize\n","sentence_to_vectorize = \"The quick brown fox jumps over the lazy dog.\"\n","bow_vector = vectorize_bow(sentence_to_vectorize, vocab)\n","print(f\"\\nBoW vector for '{sentence_to_vectorize}': {bow_vector}\")\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique VocabList: ['absolutely', 'algebra', 'artificial', 'back', 'behind', 'brown', 'concern', 'data', 'delicious', 'dog', 'ethical', 'fox', 'go', 'hate', 'however', 'intelligence', 'involves', 'jump', 'lazy', 'learning', 'linear', 'love', 'machine', 'math', 'mind', 'nobler', 'pizza', 'question', 'quick', 'remain', 'science', 'service', 'statistic', 'terrible', 'transforming', 'whether', 'wo', 'world']\n","\n","BoW vector for 'The quick brown fox jumps over the lazy dog.': [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**2.2: BoW Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.CountVectorizer.\n","\n","**Steps:**\n","\n","1. Instantiate the vectorizer.\n","\n","2. fit_transform the raw corpus.\n","\n","3. Convert the result to an array (.toarray()) and print it."],"metadata":{"id":"UwsoZix-zUDC"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Instantiate vectorizer\n","vectorizer = CountVectorizer()\n","\n","# Fit and transform row corpus\n","bow_matrix = vectorizer.fit_transform(corpus)\n","\n","# Convert to array\n","print(bow_matrix.toarray())\n","\n","\n"],"metadata":{"id":"RGs7EzLRzfGC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765362417009,"user_tz":-330,"elapsed":18,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"1addfa7f-dd29-4572-b0ad-c66694a5949f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1]\n"," [1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n","  0 0 0 0 1 0 1 0 2 0 0 0 2 0 1 0]\n"," [0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0\n","  0 1 0 0 0 0 0 0 2 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0\n","  1 0 0 0 0 0 0 1 2 1 2 0 0 1 0 0]\n"," [0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n","  0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n"]}]},{"cell_type":"markdown","source":["**2.3: TF-IDF From Scratch (The Math)**\n","\n","**Goal:** Manually calculate the score for the word \"machine\" in the last sentence:\n","\n","\"I love machine learning, but I hate the math behind it.\"\n","\n","**Formula:**\n","\n","*TF (Term Frequency):* $\\frac{\\text{Count of 'machine' in sentence}}{\\text{Total words in sentence}}$\n","\n","*IDF (Inverse Document Frequency):* $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing 'machine'}})$ (Use math.log).\n","\n","**Result:** TF * IDF.\n","\n","**Task:** Print your manual calculation result."],"metadata":{"id":"-MR6Bxgh0Gpu"}},{"cell_type":"code","source":["import math\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","\n","def clean_text_nltk(text):\n","\n","    text = text.lower()\n","\n","    tokens = word_tokenize(text)\n","\n","\n","    tokens = [word for word in tokens if word.isalpha()]\n","\n","    stop_words_nltk = set(stopwords.words('english'))\n","\n","\n","    tokens = [word for word in tokens if word not in stop_words_nltk]\n","\n","    lemmatizer = WordNetLemmatizer()\n","\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","    return tokens\n","\n","# Target word and sentence\n","target_word = \"machine\"\n","sentence_index = 5\n","target_sentence = corpus[sentence_index]\n","\n","print(f\" target_word: '{target_word}'\")\n","print(f\" target_sentence:'{target_sentence}'\")\n","\n","# 1. TF calculations\n","\n","cleaned_targetsen = clean_text_nltk(target_sentence)\n","\n","print(f\"Cleaned tokens: {cleaned_targetsen}\")\n","\n","target_word_in_sent = cleaned_targetsen.count(target_word)\n","\n","total_words = len(cleaned_targetsen)\n","\n","if total_words > 0:\n","    tf = target_word_in_sent / total_words\n","else:\n","    tf = 0.0\n","\n","print(f\"Count of '{target_word}' : {target_word_in_sent}\")\n","\n","print(f\" words in cleaned sentence: {total_words}\")\n","\n","print(f\"TF for '{target_word}': {tf:.4f}\")\n","\n","\n","\n","\n","#  2. idf calculation\n","\n","total_doc = len(corpus)\n","\n","doc_containing_machine = 0\n","\n","for doc_idx, doc_text in enumerate(corpus):\n","\n","     cleaned_doc = clean_text_nltk(doc_text)\n","     if target_word in cleaned_doc:\n","\n","        doc_containing_machine += 1\n","\n","print(f\"Total documents : {total_doc}\")\n","print(f\"Num. of doc containing '{target_word}': {doc_containing_machine}\")\n","\n","if doc_containing_machine > 0:\n","    idf = math.log(total_doc / doc_containing_machine)\n","else:\n","    idf = 0.0\n","\n","print(f\"IDF for '{target_word}': {idf:.4f}\")\n","\n","#  3. Calculate TF-IDF\n","tf_idf = tf * idf\n","\n","print(f\"TF-IDF for '{target_word}' in sentence: {tf_idf:.4f}\")\n"],"metadata":{"id":"gNSo-nza0k_c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2.4: TF-IDF Using Tools**\n","\n","**Task:** Use sklearn.feature_extraction.text.TfidfVectorizer.\n","\n","**Steps:** Fit it on the corpus and print the vector for the first sentence.\n","\n","**Observation:** Compare the score of unique words (like \"Intelligence\") vs common words (like \"is\"). Which is higher?"],"metadata":{"id":"YEYkuoSb0nDe"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#vectorizer initialization\n","vectorizer = TfidfVectorizer()\n","\n","# Fit it on corpus\n","tfidf_matrix = vectorizer.fit_transform(corpus)\n","\n","# Get the feature names (vocabulary)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Get the TF-IDF vector for the first sentence (corpus[0])\n","# The .toarray() converts the sparse matrix row to a dense numpy array\n","first_sentence_tfidf_vector = tfidf_matrix[0].toarray()[0]\n","\n","print(\"TF-IDF Vector for the first sentence:\")\n","\n","# making a dictionary\n","word_tfidf_scores = {}\n","\n","for word_idx, score in enumerate(first_sentence_tfidf):\n","      if score > 0: #\n","\n","         word_tfidf_scores[feature_names[word_idx]] = score\n","\n","# Sort by score for better comparison\n","sorted_scores = sorted(word_tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n","for word, score in sorted_scores:\n","    print(f\"  {word}: {score:.4f}\")\n"],"metadata":{"id":"Of6PfWyd0pnl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765364109016,"user_tz":-330,"elapsed":12,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"338dbc28-0cd4-47c3-ab62-f480d4ee96f8"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF Vector for the first sentence:\n","  artificial: 0.3345\n","  concerns: 0.3345\n","  ethical: 0.3345\n","  however: 0.3345\n","  intelligence: 0.3345\n","  remain: 0.3345\n","  transforming: 0.3345\n","  world: 0.3345\n","  is: 0.2743\n","  the: 0.1714\n"]}]},{"cell_type":"markdown","source":["# **Part 3- Word Embeddings**"],"metadata":{"id":"YWAar8IIzp_m"}},{"cell_type":"markdown","source":["**3.1: Word2Vec Using Tools**\n","\n","**Task:** Train a model using gensim.models.Word2Vec.\n","\n","**Steps:**\n","\n","1. Pass your cleaned tokenized corpus (from Part 1.2) to Word2Vec.\n","\n","2. Set min_count=1 (since our corpus is small, we want to keep all words).\n","\n","3. Set vector_size=10 (small vector size for easy viewing).\n","\n","**Experiment:** Print the vector for the word \"learning\"."],"metadata":{"id":"uY1URFxgz036"}},{"cell_type":"code","source":["!pip install gensim\n","from gensim.models import Word2Vec\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# ntlk download\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","#corpus uploading in model\n","tokenizedcorpus_for_word2vec = []\n","for sentence in corpus:\n","    tokenizedcorpus_for_word2vec.append(clean_text_nltk(sentence))\n","\n","# Training model\n","model = Word2Vec(sentences=processed_corpus_for_word2vec, vector_size=10, min_count=1, workers=4)\n","\n","# Experiment: Print the vector for the word \"has\"\n","word = \"has\"\n","\n","if word in model.wv:\n","\n","    print(f\"Vector of'{word}':\")\n","\n","    print(model.wv[word])\n","else:\n","    print(f\"'{word}' not found .\")\n"],"metadata":{"id":"aziX2IGBzyaa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765365558042,"user_tz":-330,"elapsed":5829,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"b6cbe14b-a205-49c4-a8b0-e5ca73927002"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","Vector of'learning':\n","[-0.00535678  0.00238785  0.05107836  0.09016657 -0.09301379 -0.07113771\n","  0.06464887  0.08973394 -0.05023384 -0.03767424]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["**3.3: Pre-trained GloVe (Understanding Global Context)**\n","\n","**Task:** Use gensim.downloader to load 'glove-wiki-gigaword-50'\n","\n","**Analogy Task:** Compute the famous analogy:$\\text{King} - \\text{Man} + \\text{Woman} = ?$\n","\n","Use model.most_similar(positive=['woman', 'king'], negative=['man']).\n","\n","**Question:** Does the model correctly guess \"Queen\"?"],"metadata":{"id":"r3J42eQZ1fUo"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","# Load pre-trained GloVe model with the correct name\n","glove_model = api.load('glove-wiki-gigaword-50')\n","\n","# Analogy Task: King - Man + Woman = ?\n","# Use model.most_similar(positive=['woman', 'king'], negative=['man'])\n","analogy_result = glove_model.most_similar(positive=['woman', 'king'], negative=['man'])\n","\n","print(\"Analogy: King - Man + Woman = ?\")\n","print(analogy_result)\n","\n","# Check if the model correctly guesses \"Queen\"\n","if analogy_result and analogy_result[0][0].lower() == 'queen':\n","    print(\"\\nObservation: Yes, the model correctly guesses 'Queen' as the top result.\")\n","else:\n","    print(\"\\nObservation: The model did not guess 'Queen' as the top result or there was no result. The top result is: \" + analogy_result[0][0])\n"],"metadata":{"id":"LEj5SkO81mkF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765365174912,"user_tz":-330,"elapsed":33221,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"df520432-0321-49c0-b9c4-b1329b9c8fd8"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Analogy: King - Man + Woman = ?\n","[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904), ('princess', 0.7424570322036743), ('kingdom', 0.7337412238121033), ('monarch', 0.721449077129364), ('eldest', 0.7184861898422241), ('widow', 0.7099431157112122)]\n","\n","Observation: Yes, the model correctly guesses 'Queen' as the top result.\n"]}]},{"cell_type":"markdown","source":["# **Part 5- Sentiment Analysis (The Application)**\n","\n","**Concept:** Sentiment Analysis determines whether a piece of text is Positive, Negative, or Neutral. We will use VADER (Valence Aware Dictionary and sEntiment Reasoner) from NLTK. VADER is specifically designed for social media text; it understands that capital letters (\"LOVE\"), punctuation (\"!!!\"), and emojis change the sentiment intensity.\n","\n","**Task:**\n","\n","1. Initialize the SentimentIntensityAnalyzer.\n","\n","2. Pass the Pizza Review (corpus[1]) into the analyzer.\n","\n","3. Pass the Math Complaint (corpus[5]) into the analyzer.\n","\n","**Analysis:** Look at the compound score for both.\n","\n","**Compound Score Range:** -1 (Most Negative) to +1 (Most Positive).\n","\n","Does the model correctly identify that \"delicious\" and \"terrible\" in the same sentence result in a mixed or neutral score?"],"metadata":{"id":"AbI4K0UJUxy3"}},{"cell_type":"code","source":["import nltk\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","\n","# donload vader\n","nltk.download('vader_lexicon')\n","\n","#   initiating sentimentintensityanalyzer\n","sia = SentimentIntensityAnalyzer()\n","\n","#  pizza review of a sentence 2\n","pizza_review = corpus[1]\n","print(f\"'{pizza_review}'\")\n","\n","pizza_sent = sia.polarity_scores(pizza_review)\n","print(f\"Sentiment: {pizza_sent}\")\n","\n","#  Mth complaint (corpus[5])\n","\n","\n","\n","math_complaint = corpus[5]\n","\n","\n","print(f\"math complaint: '{math_complaint}'\")\n","\n","\n","mth_sent = sia.polarity_scores(math_complaint)\n","\n","\n","print(f\"sentiment for mth complaint: {mth_sent}\")\n","\n","print(f\" Review cscore: {pizza_sent['compound']:.2f}\")\n","print(f\" complaint cscore: {mth_sent['compound']:.2f}\")\n","\n","\n","if pizza_sentiment['compound'] >= -0.05 and pizza_sentiment['compound'] <= 0.05:\n","\n","\n","    print(\"obsrvn: The model correctly identifies the pizza review.\")\n","else:\n","    print(\"obsrvn: The model did not identify the pizza review .\")\n"],"metadata":{"id":"_lC2c3GHUxU-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765366104655,"user_tz":-330,"elapsed":5,"user":{"displayName":"Khushi Yadav","userId":"15476497084190940231"}},"outputId":"25924116-b4ef-4d3e-8a0d-7cd28e515b9a"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["'The pizza was absolutely delicious, but the service was terrible ... I won't go back.'\n","Sentiment: {'neg': 0.223, 'neu': 0.644, 'pos': 0.134, 'compound': -0.3926}\n","math complaint: 'I love machine learning, but I hate the math behind it.'\n","sentiment for mth complaint: {'neg': 0.345, 'neu': 0.478, 'pos': 0.177, 'compound': -0.5346}\n"," Review cscore: -0.39\n"," complaint cscore: -0.53\n","obsrvn: The model did not identify the pizza review .\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]}]}